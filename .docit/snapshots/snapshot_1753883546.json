{
  "doc_it\\main.py": "# doc_it/main.py\n\nimport typer\n\n# Import the Typer app objects from the command files\nfrom .commands.init import app as init_app\nfrom .commands.commit import app as commit_app # NEW: Import the commit app\n\n# Create the main Typer application\napp = typer.Typer(\n    name=\"doc-it\",\n    help=\"A modular CLI tool to automatically generate documentation for code changes using a local Ollama LLM.\",\n    add_completion=False\n)\n\n# Register the command modules\napp.add_typer(init_app, name=\"init\")\napp.add_typer(commit_app, name=\"commit\") # NEW: Register the commit command\n\n\n# This is the entry point that Typer calls when you run 'doc-it'\nif __name__ == \"__main__\":\n    app()\n",
  "doc_it\\__init__.py": "",
  "doc_it\\commands\\commit.py": "# doc_it/commands/commit.py\n\nimport json\nimport typer\nfrom rich.console import Console\n\n# Import all the necessary utility and database functions\nfrom doc_it.utils.diff_util import (\n    take_snapshot,\n    get_latest_snapshot_path,\n    compare_snapshots,\n)\nfrom doc_it.utils.llm_util import generate_change_explanation\nfrom doc_it.db.database import get_codebase_summary, save_explanation\n\nconsole = Console()\napp = typer.Typer()\n\n@app.callback(invoke_without_command=True)\ndef commit_command(\n    message: str = typer.Option(..., \"-m\", \"--message\", help=\"A message describing the changes made.\")\n):\n    \"\"\"\n    Analyzes code changes since the last snapshot, generates AI explanations,\n    and saves them to the database.\n    \"\"\"\n    console.print(f\"\\n[bold cyan]Analyzing changes for commit: '{message}'[/bold cyan]\")\n\n    # 1. Find the most recent snapshot to compare against.\n    old_snapshot_path = get_latest_snapshot_path()\n    if not old_snapshot_path:\n        console.print(\"[bold red]Error: No previous snapshot found. Please run 'doc-it init' first.[/bold red]\")\n        raise typer.Exit(code=1)\n\n    # 2. Take a new snapshot of the project's current state.\n    console.print(\"Taking new snapshot of codebase...\")\n    new_snapshot_path = take_snapshot()\n    if not new_snapshot_path:\n        console.print(\"[yellow]No code files found to snapshot. Nothing to commit.[/yellow]\")\n        raise typer.Exit()\n\n    # 3. Compare the two snapshots to find what has changed.\n    console.print(\"Comparing snapshots to find changes...\")\n    with open(old_snapshot_path, 'r') as f:\n        old_data = json.load(f)\n    with open(new_snapshot_path, 'r') as f:\n        new_data = json.load(f)\n\n    changes = compare_snapshots(old_snapshot_path, new_snapshot_path)\n    modified_files = changes.get(\"modified\", [])\n    added_files = changes.get(\"added\", [])\n    # We can also track deleted files if needed in the future\n    # deleted_files = changes.get(\"deleted\", [])\n\n    if not modified_files and not added_files:\n        console.print(\"\\n[green]No changes detected. Everything is up-to-date.[/green]\")\n        raise typer.Exit()\n\n    # 4. Get the codebase summary to give the AI context.\n    codebase_summary = get_codebase_summary()\n    if not codebase_summary:\n        console.print(\"[yellow]Warning: Could not retrieve codebase summary for context.[/yellow]\")\n        codebase_summary = \"A software project.\" # Fallback summary\n\n    # 5. Iterate over each changed file and generate an explanation.\n    all_changes = ((\"modified\", modified_files, \"Analyzing modified file\"), \n                   ((\"added\", added_files, \"Analyzing added file\")))\n\n    for change_type, file_list, message_text in all_changes:\n        for filepath in file_list:\n            console.print(f\"-> {message_text}: [cyan]{filepath}[/cyan]\")\n            old_code = old_data.get(filepath, \"\") if change_type == \"modified\" else \"\"\n            new_code = new_data.get(filepath, \"\")\n            \n            explanation = generate_change_explanation(filepath, old_code, new_code, codebase_summary)\n            if explanation:\n                save_explanation(filepath, explanation)\n                console.print(f\"   [green]\u2713 Explanation saved.[/green]\")\n            else:\n                console.print(f\"   [red]\u2717 Failed to generate explanation.[/red]\")\n\n    console.print(\"\\n[bold green]Change analysis complete![/bold green]\")\n\n",
  "doc_it\\commands\\init.py": "# doc_it/commands/init.py\n\nimport os\nfrom pathlib import Path\nimport typer\nfrom rich.console import Console\n\n# Import our utility and database functions\nfrom doc_it.utils.config_handler import (\n    PROJECT_ROOT,\n    DEFAULT_CONFIG,\n    ensure_docit_dir_exists,\n    save_config,\n    get_config,\n)\nfrom doc_it.db.database import initialize, save_codebase_summary\nfrom doc_it.utils.llm_util import generate_codebase_summary\nfrom doc_it.utils.diff_util import take_snapshot # NEW: Import take_snapshot\n\nconsole = Console()\napp = typer.Typer()\n\ndef collect_codebase_content() -> str:\n    \"\"\"\n    Scans the project directory and collects the content of all files\n    specified in the configuration.\n    \"\"\"\n    console.print(\"[cyan]Scanning project files...[/cyan]\")\n    config = get_config()\n    files_to_include = config.get(\"files_to_include\", [])\n    files_to_ignore = config.get(\"files_to_ignore\", [])\n    \n    all_content = []\n    \n    for pattern in files_to_include:\n        for filepath in PROJECT_ROOT.rglob(pattern):\n            if any(part in files_to_ignore for part in filepath.parts):\n                continue\n\n            try:\n                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                    content = f.read()\n                    all_content.append(f\"--- File: {filepath.relative_to(PROJECT_ROOT)} ---\\n{content}\\n\")\n            except Exception as e:\n                console.print(f\"[yellow]Could not read file {filepath}: {e}[/yellow]\")\n\n    return \"\\n\".join(all_content)\n\n@app.callback(invoke_without_command=True)\ndef init_command():\n    \"\"\"\n    Initializes a new project for Doc-It.\n    - Creates the .docit directory and config file.\n    - Initializes the database.\n    - Scans the codebase and generates an initial AI summary.\n    - Takes the initial snapshot.\n    \"\"\"\n    console.print(\"[bold green]Initializing Doc-It for this project...[/bold green]\")\n\n    ensure_docit_dir_exists()\n    save_config(DEFAULT_CONFIG)\n    console.print(\"\u2705 Created .docit directory and config file.\")\n\n    if initialize():\n        console.print(\"\u2705 Database initialized successfully.\")\n    else:\n        console.print(\"[bold red]Error: Failed to initialize the database.[/bold red]\")\n        raise typer.Exit(code=1)\n\n    codebase_content = collect_codebase_content()\n    if not codebase_content:\n        console.print(\"[yellow]Warning: No source code files found to summarize.[/yellow]\")\n        return\n\n    summary = generate_codebase_summary(codebase_content)\n    if not summary:\n        console.print(\"[bold red]Error: Could not generate codebase summary.[/bold red]\")\n        raise typer.Exit(code=1)\n\n    save_codebase_summary(summary)\n    console.print(\"\u2705 AI-generated codebase summary has been created and saved.\")\n    \n    # --- THIS IS THE FIX ---\n    # Take the initial snapshot of the codebase after setup.\n    console.print(\"Taking initial snapshot...\")\n    if take_snapshot():\n        console.print(\"\u2705 Initial snapshot created.\")\n    else:\n        console.print(\"[yellow]Warning: Could not create initial snapshot.[/yellow]\")\n\n    console.print(\"\\n[bold green]Doc-It initialization complete![/bold green]\")\n",
  "doc_it\\commands\\__init__.py": "",
  "doc_it\\db\\database.py": "# doc_it/db/database.py\n\nimport sqlite3\nfrom rich.console import Console\nfrom doc_it.utils.config_handler import DB_FILE # Import the DB_FILE path\n\nconsole = Console()\n\ndef get_connection():\n    \"\"\"Establishes a connection to the SQLite database.\"\"\"\n    try:\n        # Using check_same_thread=False is generally safe for CLI tools\n        # where operations are sequential.\n        con = sqlite3.connect(DB_FILE, check_same_thread=False)\n        return con\n    except sqlite3.Error as e:\n        console.print(f\"[bold red]Database connection error: {e}[/bold red]\")\n        return None\n\ndef initialize():\n    \"\"\"\n    Initializes the database and creates the three required tables if they don't exist.\n    This function is safe to run multiple times.\n    \"\"\"\n    con = get_connection()\n    if con is None:\n        return False\n        \n    try:\n        with con:\n            cur = con.cursor()\n            # Table 1: For storing the high-level summary of the entire codebase\n            cur.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS codebase_summary (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    summary TEXT NOT NULL,\n                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            # Table 2: For storing explanations of changes to specific files\n            cur.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS explanations (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    filepath TEXT NOT NULL,\n                    explanation TEXT NOT NULL,\n                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            # Table 3: For logging detailed file changes (can be used later)\n            cur.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS changes (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    filepath TEXT NOT NULL,\n                    change_type TEXT NOT NULL,\n                    old_code TEXT,\n                    new_code TEXT,\n                    explanation_id INTEGER,\n                    user_message TEXT,\n                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n                    FOREIGN KEY (explanation_id) REFERENCES explanations (id)\n                )\n            \"\"\")\n        return True\n    except sqlite3.Error as e:\n        console.print(f\"[bold red]Error initializing database tables: {e}[/bold red]\")\n        return False\n    finally:\n        if con:\n            con.close()\n\ndef save_codebase_summary(summary: str):\n    \"\"\"Saves the given codebase summary to the database.\"\"\"\n    con = get_connection()\n    if con is None: return\n    try:\n        with con:\n            cur = con.cursor()\n            cur.execute(\"INSERT INTO codebase_summary (summary) VALUES (?)\", (summary,))\n    except sqlite3.Error as e:\n        console.print(f\"[bold red]Error saving codebase summary: {e}[/bold red]\")\n    finally:\n        if con: con.close()\n\ndef get_codebase_summary() -> str | None:\n    \"\"\"Retrieves the most recent codebase summary from the database.\"\"\"\n    con = get_connection()\n    if con is None: return None\n    try:\n        with con:\n            cur = con.cursor()\n            # Get the most recent summary, as it's the most relevant one\n            cur.execute(\"SELECT summary FROM codebase_summary ORDER BY timestamp DESC LIMIT 1\")\n            result = cur.fetchone()\n            return result[0] if result else None\n    except sqlite3.Error as e:\n        console.print(f\"[bold red]Error retrieving codebase summary: {e}[/bold red]\")\n        return None\n    finally:\n        if con: con.close()\n\ndef save_explanation(filepath: str, explanation: str) -> int | None:\n    \"\"\"Saves a file change explanation and returns its unique ID.\"\"\"\n    con = get_connection()\n    if con is None: return None\n    try:\n        with con:\n            cur = con.cursor()\n            cur.execute(\"INSERT INTO explanations (filepath, explanation) VALUES (?, ?)\", (filepath, explanation))\n            return cur.lastrowid\n    except sqlite3.Error as e:\n        console.print(f\"[bold red]Error saving explanation: {e}[/bold red]\")\n        return None\n    finally:\n        if con: con.close()\n\ndef get_explanations() -> list[dict]:\n    \"\"\"Retrieves all stored explanations, returning them as a list of dictionaries.\"\"\"\n    con = get_connection()\n    if con is None: return []\n    try:\n        with con:\n            con.row_factory = sqlite3.Row # This allows accessing columns by name\n            cur = con.cursor()\n            cur.execute(\"SELECT filepath, explanation, timestamp FROM explanations ORDER BY timestamp ASC\")\n            rows = cur.fetchall()\n            # Convert the sqlite3.Row objects to standard Python dictionaries\n            return [dict(row) for row in rows]\n    except sqlite3.Error as e:\n        console.print(f\"[bold red]Error retrieving explanations: {e}[/bold red]\")\n        return []\n    finally:\n        if con: con.close()\n",
  "doc_it\\db\\__init__.py": "",
  "doc_it\\utils\\config_handler.py": "# doc_it/utils/config_handler.py\n\nimport json\nfrom pathlib import Path\n\n# --- Path Definitions ---\n# Find the project root by looking for a .git folder. This makes the tool\n# work correctly even if you run it from a sub-directory.\ntry:\n    PROJECT_ROOT = Path.cwd().joinpath('.git').resolve().parent\nexcept FileNotFoundError:\n    # As a fallback, use the current working directory if not in a git repo\n    PROJECT_ROOT = Path.cwd()\n\n# All of Doc-It's internal files will be stored in a hidden .docit directory\nDOCIT_DIR = PROJECT_ROOT / \".docit\"\nCONFIG_FILE = DOCIT_DIR / \"config.json\"\nDB_FILE = DOCIT_DIR / \"docit.db\"\n\n\n# --- Default Configuration ---\n# These are the settings that will be created during the 'init' command.\nDEFAULT_CONFIG = {\n    \"author\": \"Your Name\",\n    \"ollama_model_name\": \"llama3:latest\",\n    \"ollama_api_endpoint\": \"http://localhost:11434/api/generate\",\n    \"files_to_include\": [\"*.py\", \"*.js\", \"*.html\", \"*.css\"],\n    \"files_to_ignore\": [\"__pycache__\", \".venv\", \".docit\"]\n}\n\n\n# --- Core Functions ---\ndef ensure_docit_dir_exists():\n    \"\"\"Creates the .docit directory in the project root if it doesn't already exist.\"\"\"\n    DOCIT_DIR.mkdir(exist_ok=True)\n\ndef get_config() -> dict:\n    \"\"\"Loads the configuration from the JSON file. Returns default if not found.\"\"\"\n    if not CONFIG_FILE.exists():\n        return DEFAULT_CONFIG\n    with open(CONFIG_FILE, 'r') as f:\n        return json.load(f)\n\ndef save_config(config_data: dict):\n    \"\"\"Saves the given dictionary to the config.json file.\"\"\"\n    ensure_docit_dir_exists()\n    with open(CONFIG_FILE, 'w') as f:\n        json.dump(config_data, f, indent=4)\n",
  "doc_it\\utils\\diff_util.py": "# doc_it/utils/diff_util.py\n\nimport difflib\nimport json\nimport time\nfrom pathlib import Path\nfrom rich.console import Console\n\nfrom .config_handler import PROJECT_ROOT, DOCIT_DIR, get_config\n\nconsole = Console()\n\nSNAPSHOTS_DIR = DOCIT_DIR / \"snapshots\"\n\ndef take_snapshot() -> Path | None:\n    \"\"\"\n    Scans the current codebase, creates a snapshot of file contents,\n    and saves it to a timestamped JSON file.\n\n    Returns:\n        The path to the newly created snapshot file, or None on error.\n    \"\"\"\n    SNAPSHOTS_DIR.mkdir(exist_ok=True)\n    config = get_config()\n    files_to_include = config.get(\"files_to_include\", [])\n    files_to_ignore = config.get(\"files_to_ignore\", [])\n    \n    snapshot_data = {}\n    \n    for pattern in files_to_include:\n        for filepath in PROJECT_ROOT.rglob(pattern):\n            if any(part in files_to_ignore for part in filepath.parts):\n                continue\n            \n            try:\n                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                    content = f.read()\n                    snapshot_data[str(filepath.relative_to(PROJECT_ROOT))] = content\n            except Exception:\n                pass\n\n    if not snapshot_data:\n        return None\n\n    timestamp = int(time.time())\n    snapshot_path = SNAPSHOTS_DIR / f\"snapshot_{timestamp}.json\"\n    \n    with open(snapshot_path, 'w') as f:\n        json.dump(snapshot_data, f, indent=2)\n        \n    return snapshot_path\n\n\ndef get_latest_snapshot_path() -> Path | None:\n    \"\"\"Finds the most recent snapshot file in the snapshots directory.\"\"\"\n    if not SNAPSHOTS_DIR.exists():\n        return None\n    \n    snapshots = list(SNAPSHOTS_DIR.glob(\"snapshot_*.json\"))\n    if not snapshots:\n        return None\n        \n    return max(snapshots, key=lambda p: p.stat().st_mtime)\n\n\ndef compare_snapshots(old_snapshot_path: Path, new_snapshot_path: Path) -> dict:\n    \"\"\"\n    Compares two snapshots and identifies added, modified, and deleted files.\n    \"\"\"\n    with open(old_snapshot_path, 'r') as f:\n        old_data = json.load(f)\n    with open(new_snapshot_path, 'r') as f:\n        new_data = json.load(f)\n\n    old_files = set(old_data.keys())\n    new_files = set(new_data.keys())\n\n    return {\n        \"added\": list(new_files - old_files),\n        \"deleted\": list(old_files - new_files),\n        \"modified\": [\n            f for f in old_files & new_files if old_data[f] != new_data[f]\n        ],\n    }\n\ndef get_code_diff(old_content: str, new_content: str) -> str:\n    \"\"\"\n    Generates a unified diff string between two versions of a file's content.\n    \"\"\"\n    diff = difflib.unified_diff(\n        old_content.splitlines(keepends=True),\n        new_content.splitlines(keepends=True),\n        fromfile='old',\n        tofile='new',\n    )\n    return \"\".join(diff)\n",
  "doc_it\\utils\\llm_util.py": "# doc_it/utils/llm_util.py\n\nimport json\nimport requests\nfrom rich.console import Console\nfrom rich.spinner import Spinner\n\nfrom .config_handler import get_config\n\nconsole = Console()\n\ndef send_to_ollama(prompt: str, model_name: str, api_endpoint: str) -> str | None:\n    \"\"\"\n    Sends a prompt to the specified Ollama API endpoint and streams the response.\n\n    Args:\n        prompt: The full prompt to send to the LLM.\n        model_name: The name of the Ollama model to use (e.g., 'llama3:latest').\n        api_endpoint: The URL of the Ollama generate API.\n\n    Returns:\n        The complete generated response as a string, or None if an error occurs.\n    \"\"\"\n    try:\n        payload = {\n            \"model\": model_name,\n            \"prompt\": prompt,\n            \"stream\": False  # We'll get the full response at once\n        }\n\n        with console.status(f\"[bold cyan]Asking '{model_name}' for insights...[/bold cyan]\", spinner=\"dots\") as status:\n            response = requests.post(api_endpoint, json=payload)\n            response.raise_for_status()  # Will raise an HTTPError for bad responses (4xx or 5xx)\n\n            response_data = response.json()\n            return response_data.get(\"response\", \"\").strip()\n\n    except requests.exceptions.ConnectionError:\n        console.print(f\"[bold red]Error: Could not connect to Ollama API at '{api_endpoint}'.[/bold red]\")\n        console.print(\"Please ensure Ollama is running and accessible.\")\n        return None\n    except requests.exceptions.RequestException as e:\n        console.print(f\"[bold red]An error occurred while communicating with Ollama: {e}[/bold red]\")\n        return None\n\n\ndef generate_codebase_summary(codebase_content: str) -> str | None:\n    \"\"\"\n    Asks the LLM to generate a high-level summary of the entire codebase.\n\n    Args:\n        codebase_content: A single string containing the content of all relevant files.\n\n    Returns:\n        The AI-generated summary as a string.\n    \"\"\"\n    config = get_config()\n    model_name = config.get(\"ollama_model_name\")\n    api_endpoint = config.get(\"ollama_api_endpoint\")\n\n    prompt = (\n        \"You are an expert software architect. Below is the content of an entire codebase. \"\n        \"Your task is to provide a high-level summary of the project.\\n\"\n        \"Describe its main purpose, the technologies used, and the overall structure. \"\n        \"Keep the summary concise and to the point.\\n\\n\"\n        \"--- CODEBASE ---\\n\"\n        f\"{codebase_content}\\n\"\n        \"--- END CODEBASE ---\\n\\n\"\n        \"Summary:\"\n    )\n    \n    console.print(\"\\n[cyan]Generating codebase summary...[/cyan]\")\n    return send_to_ollama(prompt, model_name, api_endpoint)\n\n\ndef generate_change_explanation(filepath: str, old_code: str, new_code: str, codebase_summary: str) -> str | None:\n    \"\"\"\n    Asks the LLM to explain the changes made to a single file.\n\n    Args:\n        filepath: The path to the file that was changed.\n        old_code: The content of the file before the change.\n        new_code: The content of the file after the change.\n        codebase_summary: The high-level summary of the project for context.\n\n    Returns:\n        The AI-generated explanation as a string.\n    \"\"\"\n    config = get_config()\n    model_name = config.get(\"ollama_model_name\")\n    api_endpoint = config.get(\"ollama_api_endpoint\")\n\n    prompt = (\n        f\"You are an expert code reviewer. The project you are reviewing has the following purpose: '{codebase_summary}'.\\n\\n\"\n        f\"A change was made to the file '{filepath}'.\\n\\n\"\n        \"--- OLD CODE ---\\n\"\n        f\"{old_code}\\n\"\n        \"--- END OLD CODE ---\\n\\n\"\n        \"--- NEW CODE ---\\n\"\n        f\"{new_code}\\n\"\n        \"--- END NEW CODE ---\\n\\n\"\n        \"Please explain this change. What was the purpose, and what was the impact? \"\n        \"Provide a clear, concise explanation suitable for documentation.\"\n    )\n\n    console.print(f\"\\n[cyan]Generating explanation for '{filepath}'...[/cyan]\")\n    return send_to_ollama(prompt, model_name, api_endpoint)\n\n",
  "doc_it\\utils\\__init__.py": "# doc_it/utils/config_handler.py\n\nimport json\nfrom pathlib import Path\n\n# --- Path Definitions ---\n# Find the project root by looking for a .git folder. This makes the tool\n# work correctly even if you run it from a sub-directory.\ntry:\n    PROJECT_ROOT = Path.cwd().joinpath('.git').resolve().parent\nexcept FileNotFoundError:\n    # As a fallback, use the current working directory if not in a git repo\n    PROJECT_ROOT = Path.cwd()\n\n# All of Doc-It's internal files will be stored in a hidden .docit directory\nDOCIT_DIR = PROJECT_ROOT / \".docit\"\nCONFIG_FILE = DOCIT_DIR / \"config.json\"\nDB_FILE = DOCIT_DIR / \"docit.db\"\n\n\n# --- Default Configuration ---\n# These are the settings that will be created during the 'init' command.\nDEFAULT_CONFIG = {\n    \"author\": \"Your Name\",\n    \"ollama_model_name\": \"llama3:latest\",\n    \"ollama_api_endpoint\": \"http://localhost:11434/api/generate\",\n    # --- THIS IS THE FIX ---\n    # We've added \"*.md\" to ensure README files are tracked.\n    \"files_to_include\": [\"*.py\", \"*.js\", \"*.html\", \"*.css\", \"*.md\"],\n    \"files_to_ignore\": [\"__pycache__\", \".venv\", \".docit\"]\n}\n\n\n# --- Core Functions ---\ndef ensure_docit_dir_exists():\n    \"\"\"Creates the .docit directory in the project root if it doesn't already exist.\"\"\"\n    DOCIT_DIR.mkdir(exist_ok=True)\n\ndef get_config() -> dict:\n    \"\"\"Loads the configuration from the JSON file. Returns default if not found.\"\"\"\n    if not CONFIG_FILE.exists():\n        return DEFAULT_CONFIG\n    with open(CONFIG_FILE, 'r') as f:\n        return json.load(f)\n\ndef save_config(config_data: dict):\n    \"\"\"Saves the given dictionary to the config.json file.\"\"\"\n    ensure_docit_dir_exists()\n    with open(CONFIG_FILE, 'w') as f:\n        json.dump(config_data, f, indent=4)\n"
}